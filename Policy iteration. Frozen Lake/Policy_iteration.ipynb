{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fsiPV8JPooE",
        "outputId": "f6b4f36b-fcfd-4588-e941-624168508044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/MyDrive/DRL. policy iteration')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Frozen_Lake import FrozenLakeEnv"
      ],
      "metadata": {
        "id": "DCeJSbzoS5fk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "GDwGoDAawHms"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How our environment works\n",
        "\n",
        "[Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/)"
      ],
      "metadata": {
        "id": "sWymQUrYZ19Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = FrozenLakeEnv()"
      ],
      "metadata": {
        "id": "aQLemruiTl5p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_all_states() #4x4 field"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoHjQ9K_TyN4",
        "outputId": "9f45cfe2-a21b-4482-d72c-9e25fbd39f82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0, 0),\n",
              " (0, 1),\n",
              " (0, 2),\n",
              " (0, 3),\n",
              " (1, 0),\n",
              " (1, 1),\n",
              " (1, 2),\n",
              " (1, 3),\n",
              " (2, 0),\n",
              " (2, 1),\n",
              " (2, 2),\n",
              " (2, 3),\n",
              " (3, 0),\n",
              " (3, 1),\n",
              " (3, 2),\n",
              " (3, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = (1, 2)\n",
        "env.get_possible_actions(state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sim1mwuWT4FI",
        "outputId": "26a50a83-a64d-4f7b-ce4e-f76d63ade565"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('left', 'down', 'right', 'up')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Y7GpnyWg-i",
        "outputId": "10e087b8-daad-4e03-badc-b9d841cfc607"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*FFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = (3, 2)\n",
        "action = 'right'\n",
        "env.get_next_states(state, action)\n",
        "\n",
        "#(1, 1) with prob 0.1\n",
        "#(0, 2) with prob 0.8\n",
        "#stay in the same with prob 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTtqmi67WvWN",
        "outputId": "ea6f1a1f-7da4-4142-c512-73d07903955f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(3, 2): 0.1, (3, 3): 0.8, (2, 2): 0.1}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_state = (3, 3)\n",
        "env.get_transition_prob(state, action, next_state)\n",
        "\n",
        "#prob of moving to the 'next_state' from 'state' with such action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVq_Y-joW64M",
        "outputId": "f92d8909-91dd-43d4-aebd-e63d63d9b9b4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.get_reward(state, action, next_state)\n",
        "\n",
        "#positive reward only if we reach finish point (3, 3) else 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9_-ylwfXf1e",
        "outputId": "94eb5ed7-1e23-4d39-ee03-d65ace3f7235"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = (3,3)\n",
        "\n",
        "env.is_terminal(state)\n",
        "\n",
        "#true if it is last state or hole"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjfbYFDyZBBv",
        "outputId": "c28c865d-2813-4e1f-8c86-007c84632cba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Algorithm"
      ],
      "metadata": {
        "id": "5QgP-i2BZlLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If reward don not depend on s' - next state:\n",
        "$$\n",
        "q(s, a) = R(s, a) + \\gamma \\sum_{s'}P(s'|s, a) v(s')\n",
        "$$\n",
        "If reward depends on s': (our case)\n",
        "$$\n",
        "q(s, a) = \\sum_{s'} P(s'|s, a) \\Big( R(s, a, s') + \\gamma  v(s')\\Big)\n",
        "$$\n",
        "\n",
        "If reward do not depends on s' - it is special case of the dependece on s'\n",
        "\n",
        "$$\n",
        "q(s, a) = \\sum_{s'} P(s'|s, a) \\Big( R(s, a) + \\gamma  v(s')\\Big) = R(s, a) \\sum_{s'} P(s'|s,a) + \\gamma \\sum_{s'}P(s'|s, a) v(s') = R(s, a) + \\gamma \\sum_{s'}P(s'|s, a) v(s')\n",
        "$$"
      ],
      "metadata": {
        "id": "R8ggwDaUhnQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_q_values(v_values, gamma):\n",
        "  q_values = {}\n",
        "  for state in env.get_all_states():\n",
        "    q_values[state] = {}\n",
        "    for action in env.get_possible_actions(state):\n",
        "      q_values[state][action] = 0\n",
        "      for next_state in env.get_next_states(state, action):\n",
        "        q_values[state][action] += env.get_transition_prob(state, action, next_state) * env.get_reward(state, action, next_state)\n",
        "        q_values[state][action] += gamma * env.get_transition_prob(state, action, next_state) * v_values[next_state]\n",
        "\n",
        "  return q_values"
      ],
      "metadata": {
        "id": "JDcmcv6EZ7kS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_policy():\n",
        "  policy = {}\n",
        "  for state in env.get_all_states():\n",
        "    policy[state] = {}\n",
        "    for action in env.get_possible_actions(state):\n",
        "      policy[state][action] = 1 / len(env.get_possible_actions(state)) #uniform filling\n",
        "\n",
        "  return policy"
      ],
      "metadata": {
        "id": "iHWP1kYdgjrz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_v_values():\n",
        "  v_values = {}\n",
        "  for state in env.get_all_states():\n",
        "    v_values[state] = 0\n",
        "  return v_values"
      ],
      "metadata": {
        "id": "XInBAc0erpKn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can obtain $v_{\\pi}(s)$ by defining $q_{\\pi}(s, a)$ which depends on $v_{\\pi}(s')$:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "$q_{\\pi}(s, a) = R(s,a) + \\gamma \\sum_{s'} P(s'|s, a)v_{\\pi}(s')$\n",
        "\n",
        "$v_{\\pi}(s) = \\sum_{a} \\pi (a|s) q_{\\pi}(s, a)$\n"
      ],
      "metadata": {
        "id": "oHKiB2c794a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation_step(v_values, policy, gamma): #right side of bellman equation\n",
        "    q_values = get_q_values(v_values, gamma)\n",
        "    new_v_values = init_v_values()\n",
        "    for state in env.get_all_states():\n",
        "      new_v_values[state] = 0\n",
        "      for action in env.get_possible_actions(state):\n",
        "        new_v_values[state] += policy[state][action] * q_values[state][action]\n",
        "\n",
        "    return new_v_values"
      ],
      "metadata": {
        "id": "iYaxHwl0r16g"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterative Policy Evaluation:\n",
        "\n",
        "---\n",
        "\n",
        "$v^{l+1} = R_{\\pi^k} + P_{\\pi^k} v^l \\text{ for } l \\in \\overline{0, L-1}$\n",
        "\n",
        "Then we can define $q^L (s, a)$ by $v^L (s)$ (as before).\n",
        "\n"
      ],
      "metadata": {
        "id": "h-GSk-B6_JXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, gamma, eval_iter_n):\n",
        "  v_values = init_v_values()\n",
        "  for _ in range(eval_iter_n):\n",
        "    v_values = policy_evaluation_step(v_values, policy, gamma)\n",
        "  q_values = get_q_values(v_values, gamma)\n",
        "  return q_values"
      ],
      "metadata": {
        "id": "LrRj-F9roKG7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy Policy Improvement:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "$$\\pi^{k+1}(a|s) = \\begin{cases}\n",
        "  1, & \\text{if } a\\in argmax_{a' \\in A} q^L (s, a') \\\\    \n",
        "  0, & \\text{otherwise}   \n",
        "\\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "qwCSiI7tA_Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(q_values):\n",
        "  policy = {}\n",
        "  for state in env.get_all_states():\n",
        "    policy[state] = {}\n",
        "    argmax_action = None\n",
        "    max_q_value = float('-inf')\n",
        "\n",
        "    for action in env.get_possible_actions(state):\n",
        "      policy[state][action] = 0\n",
        "\n",
        "      if q_values[state][action] > max_q_value:\n",
        "        argmax_action = action\n",
        "        max_q_value = q_values[state][action]\n",
        "\n",
        "    policy[state][argmax_action] = 1\n",
        "\n",
        "  return policy"
      ],
      "metadata": {
        "id": "YpJn5WR1s6ru"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter_n = 100\n",
        "eval_iter_n = 100 #iteration of evaluation stage\n",
        "gamma = 0.99\n",
        "\n",
        "policy = init_policy()\n",
        "\n",
        "for _ in range(iter_n):\n",
        "  q_values = policy_evaluation(policy, gamma, eval_iter_n)\n",
        "  policy = policy_improvement(q_values)"
      ],
      "metadata": {
        "id": "2LFm2yjflFNt"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3VZILKlu-D6",
        "outputId": "28cb4763-cd3c-4f32-938f-b75c7dc55837"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (0, 1): {'left': 0, 'down': 0, 'right': 0, 'up': 1},\n",
              " (0, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (0, 3): {'left': 0, 'down': 0, 'right': 0, 'up': 1},\n",
              " (1, 0): {'left': 1, 'down': 0, 'right': 0, 'up': 0},\n",
              " (1, 1): {None: 1},\n",
              " (1, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (1, 3): {None: 1},\n",
              " (2, 0): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
              " (2, 1): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (2, 2): {'left': 1, 'down': 0, 'right': 0, 'up': 0},\n",
              " (2, 3): {None: 1},\n",
              " (3, 0): {None: 1},\n",
              " (3, 1): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
              " (3, 2): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
              " (3, 3): {None: 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_rewards = []\n",
        "\n",
        "for _ in range(500):\n",
        "  total_reward = 0\n",
        "  state = env.reset()\n",
        "  for _ in range(1000):\n",
        "    action = np.random.choice(env.get_possible_actions(state), p=list(policy[state].values()))\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "    #env.render()\n",
        "    #time.sleep(0.2)\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  total_rewards.append(total_reward)\n",
        "\n",
        "np.mean(total_rewards) #percent of right moves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdCUgStivbWb",
        "outputId": "dbfa6c9f-eabb-4d51-f30c-c4233970cd8e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.874"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cadj3DPNzq13",
        "outputId": "4c424357-54fb-4ac6-d7a0-c866bddb1f7c"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 0): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (0, 1): {'left': 0, 'down': 0, 'right': 0, 'up': 1},\n",
              " (0, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (0, 3): {'left': 0, 'down': 0, 'right': 0, 'up': 1},\n",
              " (1, 0): {'left': 1, 'down': 0, 'right': 0, 'up': 0},\n",
              " (1, 1): {None: 1},\n",
              " (1, 2): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (1, 3): {None: 1},\n",
              " (2, 0): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
              " (2, 1): {'left': 0, 'down': 1, 'right': 0, 'up': 0},\n",
              " (2, 2): {'left': 1, 'down': 0, 'right': 0, 'up': 0},\n",
              " (2, 3): {None: 1},\n",
              " (3, 0): {None: 1},\n",
              " (3, 1): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
              " (3, 2): {'left': 0, 'down': 0, 'right': 1, 'up': 0},\n",
              " (3, 3): {None: 1}}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}